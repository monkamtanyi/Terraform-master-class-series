**Sensitive values in state**
A local backend is backend whereby terra is storing ur statefile on ur local e't- laptop, and u are able to see d statefile.
It happens like this when we don't pass any backend configuration. Our local backend bcomes our default backend.

When we decide to configure a backend, as we initialize (terraform init) terra will ask us if we want to migrate, it will migrate
that statefile from our local e't and save it in a s3 bucket.
Note purpose of backend is to 
1) initialize uour backend
2) downloads ur modules
3) download provider plugin
4) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above



Each time u change ur backend u need to run terraform init to initialize ur backend where it is going to store d ur statefile.

When we are using an s3 as a backend be, terraform support locking that s3 with DynamoDB
so we are able to create a DynamoDB DB table and we use d DB name as a key to lock our statefile inside d s3 bucket.
Why do we lock?
State locking of statefile helps such that if u have 2 engrs. wking on same statefile (they are running d terra command or wkflow, )
we don't want more than 2 engrs writting comaands at same time on d statefele - bc statefile will be corrupted. 
Once an engr is wking on d terraform wkflow, DB locks d statefile so someone else cannot run a terraform wkflow on that statefile at
d same time- bc we want d state file to be consistent.
Once d terraform operation of d first engr is done, d statefile will unlock so someone else can perform d operation.
If u are wking on a team, the terraformstatefile will have a lock so that we do not corrupt d statefile.

Lets create an s3 bucket

1) goto : Terraform-master-class-series/08 - Terraform backend/backend/main.tf


#1. S3 bucket
resource "aws_s3_bucket" "backend" {
  count = var.create_vpc ? 1 : 0  # ? is checking if d value (var.create_vpc) is true. then it will create 1, false 0.
                                  #if u goto d variable.tf u see a variable called create_vpc - it is set to true.
                                  

  bucket = "lower(bootcamp32-${random_integer.s3.result}-${var.name}"
# for d bucket name i have used a fn,(in aws naming convention for bucket name is in lower cases)
lower fn make sure that if an engr types a cap letter in error, it transform it to lower case letters.
bc bucket name has to be unique, an integer has been used to give a random no. ( 1 to 100 herre)
it will generate that no. and concatinate d result of that no. there (random_integer.s3.result).
see d var.name under, doh in caps, it will be transformed to lower case, and that will be d bucket name- 
ken (variables.tf)



  tags = {
    Name        = "My bucket"
    Environment = "Dev"
  }
}

#2. Bucket ACL                                       #Access Control List, when u use count on a resource d outcome is a list of attributes.
resource "aws_s3_bucket_acl" "example" {             #so i will access that bucet using an index. here we want to access only d 1st bucket-
  bucket = aws_s3_bucket.backend[0].id               #and i want d id.
  acl    = var.acl
}

#3. Bucket versioning                                        #need to enable versioning on that bucket, so that ur statefile is versioned, there4
resource "aws_s3_bucket_versioning" "versioning_example" {   #our objects is versioned so that we can easily roll back.
  bucket = aws_s3_bucket.backend[0].id                      #                    
  versioning_configuration {
    status = var.versioning
  }
}

#4. Random integer
resource "random_integer" "s3" {
  max = 100
  min = 1

  keepers = {
    bucket_owner = var.name
  }
}

#5. KMS for bucket encryption                             #Key Management Service,
resource "aws_kms_key" "mykey" {
  description             = "This key is used to encrypt bucket objects"
  deletion_window_in_days = 10
}

#6. Bucket encryption
resource "aws_s3_bucket_server_side_encryption_configuration" "backend" {
  bucket = aws_s3_bucket.backend[0].id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = "aws:kms"
    }
  }
}


2) WE HAVE D OUTPUT   check in: 08 - Terraform backend/backend/output.tf
output "s3-name" {
  value = aws_s3_bucket.backend[0].bucket  #we are getting d bucket name after we have created it.
}                                           #we need this bucket name to configure our backend.

3) WE HAVE D PROVIDER    check in  BreadcrumbsTerraform-master-class-series/08 - Terraform backend/backend/provider.tf
provider "aws" {
  region = var.region
}

4) we are provisioning it in us-west-2  : check in  BreadcrumbsTerraform-master-class-series/08 - Terraform backend/backend/variable.tf  

5) WE SHALL ALSO PROVIDION OUR DYNAMODB TABLE  :08 - Terraform backend/backend/dynamoDb.tf

resource "aws_dynamodb_table" "tf_lock" {
  name           = "terraform-lock"  # the name is terraform-lock w we are going to create.
  hash_key       = "LockID"   #By setting hash_key = "LockID", you ensure that each lock
  read_capacity  = 3           # entry is uniquely identifiable and efficiently stored and retrieved.
  write_capacity = 3
  attribute {
    name = "LockID"      # d name is LockID bc we will use this table with a lockID to be able to lock our statefile.
    type = "S"         #string
  }
  tags = {
    Name = "Terraform Lock Table"
  }
  lifecycle {                   #if i want to reuse this table for many statefile, then set it to true.
    prevent_destroy = false
  }
}

PUT ALL IN A DIRECTORY CALLED backend(put 5 terraform files in this dirctory)

NOTE 
if i run d code as is, d local backend will be used for this s3 bucket bc terraform block has not been specified.
so when d statefile is created it will be stored locally - this will be a local bucket.

Bucket was created:  bootcamp32-79-ken 
above we used a locla backen bc we have our statefle captured here locally :
check in 
monka@DESKTOP-0PEDM4P MINGW64 ~/terra today/backend (master)
$ ls
dynamoDB.tf  main.tf  output.tf  provider.tf  terraform.tfstate  variable.tf

TO CREATE A REMOTE BACKEND USE
Terraform-master-class-series/08 - Terraform backend/Remote state Storage/backend-for-ec2/

Read notes for remote s3 there.


















- When you run Terraform commands with a local state file, Terraform stores the state as plain text, including variable values, even if you have flagged them as sensitive. Terraform needs to store these values in your state so that it can tell if you have changed them since the last time you applied your configuration.
- Since Terraform state can contain sensitive values, you must keep your state file secure to avoid exposing this data. Refer to the Terraform documentation to learn more about securing your state file.

**Backend**
- Each Terraform configuration can specify a backend, which defines exactly where and how operations are performed, where state snapshots are stored, etc.

- If a configuration includes no backend block, Terraform defaults to using the local backend, which performs operations on the local system and stores state as a plain file in the current working directory.

- When changing backends, Terraform will give you the option to migrate your state to the new backend. This lets you adopt backends without losing any existing state.

- You can change your backend configuration at any time. You can change both the configuration itself as well as the type of backend (for example from "consul" to "s3").

- Terraform will automatically detect any changes in your configuration and request a reinitialization. As part of the reinitialization process, Terraform will ask if you'd like to migrate your existing state to the new configuration. This allows you to easily switch from one backend to another.

**S3 Backend (with locking via DynamoDB)**
- Stores the state as a given key in a given bucket on Amazon S3. This backend also supports state locking and consistency checking via Dynamo DB, which can be enabled by setting the dynamodb_table field to an existing DynamoDB table name. A single DynamoDB table can be used to lock multiple remote state files. Terraform generates key names that include the values of the bucket and key variables.

- It is highly recommended that you enable Bucket Versioning on the S3 bucket to allow for state recovery in the case of accidental deletions and human error.

**DynamoDB State Locking**
The following configuration is optional:

**dynamodb_table**
- (Optional) Name of DynamoDB Table to use for state locking and consistency. The table must have a primary key named LockID with type of string. If not configured, state locking will be disabled.

**DynamoDB Table Permissions**
If you are using state locking, Terraform will need the following AWS IAM permissions on the DynamoDB table (arn:aws:dynamodb:::table/mytable):

     dynamodb:GetItem
     dynamodb:PutItem
     dynamodb:DeleteItem

**Data Source configurations**
- To make use of the S3 remote state in another configuration, use the terraform_remote_state data source.

```
data "terraform_remote_state" "network" {
  backend = "s3"
  config = {
    bucket = "terraform-state-prod"
    key    = "network/terraform.tfstate"
    region = "us-east-1"
  }
}
```
